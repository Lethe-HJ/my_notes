规划
```
10.10.16.55 controller1 ceph-admin  mgr、mon osd osd
10.10.16.56 controller2 ceph-node1  mgr、mon osd osd
10.10.16.58 controller3 ceph-node2  mgr、mon osd osd
10.10.16.59 compute     ceph-node3  osd osd
10.10.16.94 compute2    ceph-node4  osd osd
```

/etc/hosts
```
127.0.0.1 localhost
172.168.100.55 node1
172.168.100.56 node2
172.168.100.58 node3
172.168.100.59 node4
172.168.100.94 node5
```

然后使用hostname
hostname node1

每个节点关闭防火墙和selinux
`systemctl stop firewalld`
`systemctl disable firewalld`
`sed -i 's/SELINUX=enforcing/SELINUX=disabled/g' /etc/selinux/config`
`setenforce 0`

各节点时间同步，在[环境准备](./1.环境准备.md)中已经就绪

配置源
每个节点准备yum源
`yum clean all`
`mkdir /mnt/bak`
`mv /etc/yum.repos.d/* /mnt/bak/`

下载阿里云的base源和epel源
`wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo`
`wget -O /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-7.repo`
 
添加ceph源
`vim /etc/yum.repos.d/ceph.repo`
```
[Ceph]
name=Ceph packages for $basearch
baseurl= https://mirrors.tuna.tsinghua.edu.cn/ceph/rpm-mimic/el7/$basearch
enabled=1
gpgcheck=1
type=rpm-md
gpgkey=https://download.ceph.com/keys/release.asc
priority=1

[Ceph-noarch]
name=Ceph noarch packages
baseurl= https://mirrors.tuna.tsinghua.edu.cn/ceph/rpm-mimic/el7/noarch
enabled=1
gpgcheck=1
type=rpm-md
gpgkey=https://download.ceph.com/keys/release.asc
priority=1

[ceph-source]
name=Ceph source packages
baseurl=https://mirrors.tuna.tsinghua.edu.cn/ceph/rpm-mimic/el7/SRPMS/
enabled=1
gpgcheck=1
type=rpm-md
gpgkey=https://download.ceph.com/keys/release.asc
priority=1

```
ceph源需要拷贝到其他节点 并yum maakecache

## 初始化磁盘
在每一个节点上每一个需要部署osd的磁盘都执行以下命令
格式化磁盘

`parted -s /dev/sdb mklabel gpt mkpart primary xfs 0% 100%`

`mkfs.xfs /dev/sdb -f`

查看磁盘格式
`blkid -o value -s TYPE /dev/sdb`

输出xfs

我这里每一个节点上有两个准备部署osd的磁盘

## 部署阶段

`sudo yum update -y && sudo yum install ceph-deploy -y`

### 创建集群

`mkdir /etc/ceph`

`cd /etc/ceph`

`ceph-deploy new node1 node2 node3`

`vim ceph.conf`

```
......
public_network = 172.168.100.0/24
cluster-network = 172.168.100.0/24
osd_pool_default_size = 3
```

在各个节点上安装
yum -y install ceph ceph-radosgw

### 设置monitor和key

`ceph-deploy mon create-initial`

注意：执行完成后会在/etc/ceph目录多以下内容：

ceph.client.admin.keyring
ceph.bootstrap-mgr.keyring
ceph.bootstrap-osd.keyring
ceph.bootstrap-mds.keyring
ceph.bootstrap-rgw.keyring
ceph.bootstrap-rbd.keyring
ceph.bootstrap-rbd-mirror.keyring

将ceph.client.admin.keyring拷贝到各个节点上
`ceph-deploy admin node1 node2 node3 node4 node5`

### 安装MGR

`ceph-deploy mgr create node1 node2 node3`

### 启动osd

先在**相应节点**上手动递归建立所需文件夹  注意这里的ceph-0编号应该随着osd节点增加而增加 下一个是ceph-0 以此类推
`[root@controller1 ceph]# mkdir -vp /var/lib/ceph/osd/ceph-0`
然后
`ceph-deploy osd create --data /dev/sdb node1`
`ceph-deploy osd create --data /dev/sdc node1`
以此类推


osd节点验证

`ceph osd tree`

创建资源池
ceph osd pool create volumes 128
ceph osd pool create images 128
ceph osd pool create vms 128
ceph osd pool create backups 128


## 清理ceph环境

在各节点上执行
sudo  ps aux|grep ceph
sudo kill -9 进程号
把su cephuser和bash之外的kill了就行

su
sudo rm -rf /etc/ceph/*
sudo rm -rf /var/lib/ceph/*

sudo umount /var/lib/ceph/osd/*
sudo rm -rf /var/lib/ceph/osd/*
sudo rm -rf /var/lib/ceph/mon/*
sudo rm -rf /var/lib/ceph/mds/*
sudo rm -rf /var/lib/ceph/bootstrap-mds/*
sudo rm -rf /var/lib/ceph/bootstrap-osd/*
sudo rm -rf /var/lib/ceph/bootstrap-rgw/*
sudo rm -rf /var/lib/ceph/bootstrap-mgr/*cipan
sudo rm /home/cephuser/cluster/*
sudo pkill ceph

## ceph与openstack对接准备

2、设置ceph客户端认证，在其中一个ceph节点执行以下命令：
ceph auth get-or-create client.cinder mon 'allow r' osd 'allow class-read object_prefix rbd_children, allow rwx pool=volumes, allow rwx pool=vms, allow rwx pool=images'
ceph auth get-or-create client.glance mon 'allow r' osd 'allow class-read object_prefix rbd_children, allow rwx pool=images'
ceph auth get-or-create client.cinder-backup mon 'allow r' osd 'allow class-read object_prefix rbd_children, allow rwx pool=backups'


3、对接glance服务,执行以下操作：
ceph auth get-or-create client.glance
将输出结果存入/etc/kolla/config/glance/ceph.client.glance.keyring

对接cinder服务,执行以下操作：
ceph auth get-or-create client.cinder
将输出结果存入/etc/kolla/config/cinder/cinder-backup/ceph.client.cinder.keyring
/etc/kolla/config/cinder/cinder-volume/ceph.client.cinder.keyring
ceph auth get-or-create client.cinder-backup
将输出结果存入/etc/kolla/config/cinder/cinder-backup/ceph.client.cinder-backup.keyring

ceph auth get-or-create client.nova
将输出结果存入/etc/kolla/config/nova/ceph.client.cinder.keyring


4、将ceph.conf配置文件放置到/etc/kolla/config/cinder/，glance/，nova/,当中

5、编辑/etc/kolla/config/glance/glance-api.conf，添加如下内容
[DEFAULT]
show_image_direct_url = True

[glance_store]
stores = rbd
default_store = rbd
rbd_store_pool = images
rbd_store_user = glance
rbd_store_ceph_conf = /etc/ceph/ceph.conf


6、编辑/etc/kolla/config/cinder/cinder-volume.conf，添加如下内容
[DEFAULT]
enabled_backends=rbd-1

[rbd-1]
rbd_ceph_conf=/etc/ceph/ceph.conf
rbd_user=cinder
backend_host=rbd:volumes
rbd_pool=volumes
volume_backend_name=rbd-1
volume_driver=cinder.volume.drivers.rbd.RBDDriver
rbd_secret_uuid = {{ cinder_rbd_secret_uuid }}

备注：cinder_rbd_secret_uuid 必须能够在/etc/kolla/passwords.yml 文件找到

7、编辑/etc/kolla/config/cinder/cinder-backup.conf，添加如下内容
[DEFAULT]
backup_ceph_conf=/etc/ceph/ceph.conf
backup_ceph_user=cinder
backup_ceph_chunk_size = 134217728
backup_ceph_pool=backups
backup_driver = cinder.backup.drivers.ceph
backup_ceph_stripe_unit = 0
backup_ceph_stripe_count = 0
restore_discard_excess_bytes = true

8、编辑/etc/kolla/config/nova/nova-compute.conf，添加如下内容
[libvirt]
virt_type=qemu
images_rbd_pool=vms
images_type=rbd
images_rbd_ceph_conf=/etc/ceph/ceph.conf
rbd_user=nova

备注：rbd_user可能因您的环境而异。

9、执行kolla-ansible -i multinode reconfigure

10、重启容器

## 报错解决

### 1
如果报错
stderr: Physical volume '/dev/sdb' is already in volume group 'ceph-febe2c35-f6a9-4300-a9b9-4a4c838c3fd0'
则

dmsetup status
从DM中移除的硬盘对应的编码
dmsetup remove ceph--360db0dc--7832--46f0--9277--6c3701788334-osd--block--f3477dcf--ac71--49bb--8578--b0a6e8ef1fa7
移除分区
wipefs -a /dev/sdb
然后重新初始化磁盘

### 2 

如果报错
No such file or directory: '/var/lib/ceph/osd/ceph-2'

则去相应的节点上手动建立这个文件夹
mkdir -vp /var/lib/ceph/osd/ceph-2
然后重置相应磁盘 继续osd create

参考资料:
1.[ceph部署实践（mimic版本）](https://blog.csdn.net/wylfengyujiancheng/article/details/85613361)
2.[ceph与openstack对接](https://blog.csdn.net/ygtlovezf/article/details/78983249)
3.[kolla-ansible执行openstack对接ceph](https://my.oschina.net/jennerlo/blog/1543749)
