## 网络规划

| IP | 管理网络 | 外部网络 | 业务网络 | 存储网络 |
|:--|:--|:--|:--|:--|
| node1 | 230 | 210 | | 220 |
| node2 | 231 | 211 | | 221 |
| node3 | 232 | 212 | | 222 |
| node4 | 233 | 213 | | 223 |
| node5 | 234 | 214 | | 224 | 

## 

## hosts文件
/etc/hosts
```
127.0.0.1 localhost
192.168.80.230 node1 
192.168.80.231 node2
192.168.80.232 node3
192.168.80.233 node4
192.168.80.234 node5
192.168.80.230 registryserver
```

挂载dvd2
`mount -o loop KylinCloud-4.0.2-x86_64-DVD2-2.3.0_Build-201811070801.bin.iso /media/2.3.0`
`mount -o loop KylinCloud-4.0.2-x86_64-DVD2-2.3.9-2020032312-x86_64.iso /media/2.3.9`

展开dvd2
`cd /media/2.3.0;bash kylincloud_deploy.sh`

`cd /media/2.3.9;bash kylincloud_deploy.sh`

## 部署ceph

安装服务管理工具
`docker run --rm -v /var/run/docker.sock:/var/run/docker.sock registryserver:4000/kolla/ceph:latest setup-cmd`

确认服务管理工具部署成功
`docker exec ceph-cmd echo done`

创建并修改/etc/ceph/hosts
`vim /etc/ceph/hosts`
```
node1 192.168.80.230
node2 192.168.80.231
node3 192.168.80.232
node4 192.168.80.233
node5 192.168.80.234
```

### 分发CEPH镜像

向集群的所有主机分发容器镜像并安装服务管理工具
`docker exec ceph-cmd cluster-setup-cmd host="*"`

确认所有主机可SSH并确认服务管理工具的版本号
`docker exec ceph-cmd version host="*"`


### 生成CEPH配置文件

生成ceph.conf模板及keyring：

`docker exec ceph-cmd gen node1 node2 node3`

检查主机/etc/ceph目录下，将生成如下下列文件：
ceph.conf
ceph.client.admin.keyring
ceph.mon.keyring

根据实际环境修改ceph.conf文件：

```
# 指定各个mon的id列表，以逗号分隔
mon_initial_members = $MON_NAME_LIST
# 指定各个mon的IP地址列表，以逗号分隔 需要改成存储网的ip地址    
mon_host = $MON_ADDR_LIST
# 指定ceph的业务网地址段，处理客户端与集群的连接
# 例如: public_network = 192.168.0.0/16
public_network = $NETWORK
# 指定ceph的存储网地址段，处理集群内部各OSD之间的连接
# 例如: cluster_network = 172.19.0.0/16
# 该地址段可以与public_network配置为相同的地址段
cluster_network = $NETWORK
```

### 分发配置文件
目标为所有主机：
`docker exec ceph-cmd cluster-etc-sync host=*`

### 部署NTP
目标为所有主机：
`docker exec ceph-cmd daemon-create  host=*  ntpd`

### 部署MON节点
在管理主机上执行，例如在主机node1上部署mon，并完成三个mon节点创建：

`docker exec ceph-cmd daemon-create host=node1 mon`
`docker exec ceph-cmd daemon-create host=node2 mon`
`docker exec ceph-cmd daemon-create host=node3 mon`
确认mon状态：
`docker exec ceph-cmd ceph -s`

### 部署MGR节点
在管理主机上执行，例如在node1上部署mgr，并在所有mgr主机上部署mgr容器，创建mgr集群：
`docker exec ceph-cmd daemon-create host=node1 mgr id=node1`
`docker exec ceph-cmd daemon-create host=node2 mgr id=node2`
`docker exec ceph-cmd daemon-create host=node3 mgr id=node3`
确认mgr状态：
`docker exec ceph-cmd ceph -s`
其中，确认services状态中，mgr状态为一主多备：
mgr: node1(active), standbys: node2, node3

### 部署授权文件

确保有授权文件在/tmp/ceph.lic下

在管理主机上执行下列命令：
`cat /tmp/ceph.lic | docker exec -i ceph-cmd licence upload`

### 部署OSD节点
在所有osd主机上，为每一块用作osd的磁盘部署一个osd容器。

3.9.1手动选盘部署
在管理主机上执行，在主机node1上部署osd：
`docker exec ceph-cmd daemon-create host=node1 osd disk=/dev/sda`
`docker exec ceph-cmd daemon-create host=node1 osd disk=/dev/sdb`

`docker exec ceph-cmd daemon-create host=node2 osd disk=/dev/sda`
`docker exec ceph-cmd daemon-create host=node2 osd disk=/dev/sdb`

`docker exec ceph-cmd daemon-create host=node3 osd disk=/dev/sda`
`docker exec ceph-cmd daemon-create host=node3 osd disk=/dev/sdb`

`docker exec ceph-cmd daemon-create host=node4 osd disk=/dev/sda`
`docker exec ceph-cmd daemon-create host=node4 osd disk=/dev/sdb`

`docker exec ceph-cmd daemon-create host=node5 osd disk=/dev/sda`
`docker exec ceph-cmd daemon-create host=node5 osd disk=/dev/sdb`
如此需循环输入多次，确保所有节点、所有磁盘全部执行该操作，完成创建所有OSD的操作。

如采用SSD来提高性能，需要指定DB数据存放位置为SSD磁盘：
`docker exec ceph-cmd daemon-create host=compute1 osd disk=/dev/sdb db=/dev/sdd3`

检查所有OSD的状态：
`docker exec ceph-cmd ceph osd tree`


在管理主机上执行，例如部署所有node2上磁盘为OSD：
`docker exec ceph-cmd daemon-create-osds host=node2 dry-run=false`
检查所有OSD的状态：
`docker exec ceph-cmd ceph osd tree`

等所有OSD上线后，检查确认CEPH集群的整体状态：
`docker exec ceph-cmd ceph -s`

部署完毕后，通过命令查看ceph的运行状态，确认部署成功：
`ceph -s`

一般3副本情况下PG数量计算公式为(ceph默认备份为3副本)
这里osd_num = 10
pg=(osd_num×200)/3/3
pg约为222
2^7<222<2^8
取2^7=128

ceph osd pool create volumes 128
ceph osd pool create images 128
ceph osd pool create vms 128

通过ceph df命令可以查看我们刚刚建立的资源池状态：
`ceph df`

`cp -r /kylincloud/config/ /etc/kolla/`

`cp -rf /etc/ceph/*   /etc/kolla/config/cinder/`

`mkdir /etc/kolla/config/cinder/cinder-backup`

`mkdir /etc/kolla/config/cinder/cinder-volume`

`cp /etc/ceph/ceph.client.admin.keyring  /etc/kolla/config/cinder/cinder-backup`

`cp /etc/ceph/ceph.client.admin.keyring  /etc/kolla/config/cinder/cinder-volume`

`cp -rf /etc/ceph/*   /etc/kolla/config/glance/`

`cp -rf /etc/ceph/*   /etc/kolla/config/nova/`

`vim /etc/kolla/config/glance/glance-api.conf`

[glance_store]下面添加 rbd_store_user = admin


### multinode

`cd /kylincloud/`

`vim multinode`

`vim globals.yml`

### global.yml


### passwords.yml

修改其中的页面访问登陆密码：
`cd /kylincloud/`

`kolla-genpwd`

`vim passwords.yml`

修改文件中 keystone_admin_password 字段，设置为规划的页面访问密码，
具体配置过程如下图，将密码设置为 123123

`kolla-genpwd`

###　云平台产品认证配置
在部署节点，执行认证文件的更新操作。

`kolla-ansible -i multinode copy-license`

`kylincloud-license`


### 云平台部署实施

执行部署环境自动化检查命令。

`cd /kylincloud/`

`kolla-ansible prechecks -i multinode`

`kolla-ansible deploy -i multinode`


## 报错
osd节点down
`cmd`进入cmd
`daemon-logs host=node2 target=osd.3`查看日志
`daemon-restart host=node2 target=ods.3`重启osd



