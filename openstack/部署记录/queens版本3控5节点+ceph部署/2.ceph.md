规划
```
10.10.16.55 controller1 ceph-admin  mgr、mon osd osd
10.10.16.56 controller2 ceph-node1  mgr、mon osd osd
10.10.16.58 controller3 ceph-node2  mgr、mon osd osd
10.10.16.59 compute     ceph-node3  osd osd
10.10.16.94 compute2    ceph-node4  osd osd
```

每个节点关闭防火墙和selinux
`systemctl stop firewalld`
`systemctl disable firewalld`
`sed -i 's/SELINUX=enforcing/SELINUX=disabled/g' /etc/selinux/config`
`setenforce 0`

各节点时间同步，在[环境准备](./1.环境准备.md)中已经就绪

配置源
每个节点准备yum源
`yum clean all`
`mkdir /mnt/bak`
`mv /etc/yum.repos.d/* /mnt/bak/`

下载阿里云的base源和epel源
`wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo`
`wget -O /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-7.repo`
 
添加ceph源
`vim /etc/yum.repos.d/ceph.repo`
```
[Ceph]
name=Ceph packages for $basearch
baseurl= https://mirrors.tuna.tsinghua.edu.cn/ceph/rpm-mimic/el7/$basearch
enabled=1
gpgcheck=1
type=rpm-md
gpgkey=https://download.ceph.com/keys/release.asc
priority=1

[Ceph-noarch]
name=Ceph noarch packages
baseurl= https://mirrors.tuna.tsinghua.edu.cn/ceph/rpm-mimic/el7/noarch
enabled=1
gpgcheck=1
type=rpm-md
gpgkey=https://download.ceph.com/keys/release.asc
priority=1

[ceph-source]
name=Ceph source packages
baseurl=https://mirrors.tuna.tsinghua.edu.cn/ceph/rpm-mimic/el7/SRPMS/
enabled=1
gpgcheck=1
type=rpm-md
gpgkey=https://download.ceph.com/keys/release.asc
priority=1

```
ceph源需要拷贝到其他节点 并yum maakecache

## 初始化磁盘
在每一个节点上每一个需要部署osd的磁盘都执行以下命令
格式化磁盘

`parted -s /dev/sdb mklabel gpt mkpart primary xfs 0% 100%`

`mkfs.xfs /dev/sdb -f`

查看磁盘格式
`blkid -o value -s TYPE /dev/sdb`

输出xfs

我这里每一个节点上有两个准备部署osd的磁盘

## 部署阶段

`su - cephuser`

`sudo yum update -y && sudo yum install ceph-deploy -y`

### 创建集群

`mkdir /etc/ceph`

`cd /etc/ceph`

`ceph-deploy new controller1 controller2 controller3`

`vim ceph.conf`

```
......
public_network = 10.10.16.0/24

osd_pool_default_size = 3
```

在各个节点上安装
yum -y install ceph ceph-radosgw

### 设置monitor和key

`ceph-deploy mon create-initial`

注意：执行完成后会在/etc/ceph目录多以下内容：

ceph.client.admin.keyring
ceph.bootstrap-mgr.keyring
ceph.bootstrap-osd.keyring
ceph.bootstrap-mds.keyring
ceph.bootstrap-rgw.keyring
ceph.bootstrap-rbd.keyring
ceph.bootstrap-rbd-mirror.keyring

将ceph.client.admin.keyring拷贝到各个节点上
`ceph-deploy admin controller1 controller2 controller3 compute1 compute2`

### 安装MGR

`ceph-deploy mgr create controller1 controller2 controller3`

### 启动osd

先在**相应节点**上手动递归建立所需文件夹  注意这里的ceph-0编号应该随着osd节点增加而增加 下一个是ceph-0 以此类推
`[root@controller1 ceph]# mkdir -vp /var/lib/ceph/osd/ceph-0`
然后
`ceph-deploy osd create --data /dev/sdb controller1`
以此类推

osd节点验证

`ceph osd tree`

## 清理ceph环境

在各节点上执行
sudo  ps aux|grep ceph
sudo kill -9 进程号
把su cephuser和bash之外的kill了就行

su
sudo rm -rf /etc/ceph/*
sudo rm -rf /var/lib/ceph/*

sudo umount /var/lib/ceph/osd/*
sudo rm -rf /var/lib/ceph/osd/*
sudo rm -rf /var/lib/ceph/mon/*
sudo rm -rf /var/lib/ceph/mds/*
sudo rm -rf /var/lib/ceph/bootstrap-mds/*
sudo rm -rf /var/lib/ceph/bootstrap-osd/*
sudo rm -rf /var/lib/ceph/bootstrap-rgw/*
sudo rm -rf /var/lib/ceph/bootstrap-mgr/*cipan
sudo rm /home/cephuser/cluster/*

## 报错解决

### 1
如果报错
stderr: Physical volume '/dev/sdb' is already in volume group 'ceph-febe2c35-f6a9-4300-a9b9-4a4c838c3fd0'
则

dmsetup status
从DM中移除的硬盘对应的编码
dmsetup remove ceph--360db0dc--7832--46f0--9277--6c3701788334-osd--block--f3477dcf--ac71--49bb--8578--b0a6e8ef1fa7
移除分区
wipefs -a /dev/sdb
然后重新初始化磁盘

### 2 

如果报错
No such file or directory: '/var/lib/ceph/osd/ceph-2'

则去相应的节点上手动建立这个文件夹
mkdir -vp /var/lib/ceph/osd/ceph-2
然后重置相应磁盘 继续osd create

参考资料:
1.[ceph部署实践（mimic版本）](https://blog.csdn.net/wylfengyujiancheng/article/details/85613361)
